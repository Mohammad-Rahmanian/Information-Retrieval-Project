{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d28169",
   "metadata": {},
   "source": [
    "# Information Retrival Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4d5d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1- Positional Index Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45c2cc",
   "metadata": {},
   "source": [
    "1.1 Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66b9af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hazm\n",
    "import math\n",
    "\n",
    "with open('IR_data_news_12k.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "content_dataset, url_dataset, title_dataset = [], [], []\n",
    "for _, data in json_data.items():\n",
    "    content_dataset.append(data[\"content\"])\n",
    "    url_dataset.append(data[\"url\"])\n",
    "    title_dataset.append(data[\"title\"])\n",
    "\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "positional_index = {}\n",
    "tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b523d7",
   "metadata": {},
   "source": [
    "1.2 Preprocess Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a182bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents():\n",
    "    stop_words = hazm.stopwords_list()\n",
    "    punctuations = [')', '(', '>', '<', \"؛\", \"،\", '{', '}', \"؟\", ':', \"–\", '»', '\"', '«', '[', ']', '\"', '+', '=', '?',\n",
    "                    '/',\n",
    "                    '//', '\\\\', '|', '!', '%', '&', '*', '$', '#', '؟', '*', '.', '_', '']\n",
    "    for i in range(len(content_dataset)):\n",
    "        content_dataset[i] = normalizer.normalize(content_dataset[i])\n",
    "        first_word_tokens_list = hazm.word_tokenize(content_dataset[i])\n",
    "        second_word_tokens_list = []\n",
    "        final_word_tokens_list = []\n",
    "        for word_token in first_word_tokens_list:\n",
    "            if word_token not in stop_words and word_token not in punctuations:\n",
    "                second_word_tokens_list.append(word_token)\n",
    "        for j in range(len(second_word_tokens_list)):\n",
    "            stem_lammatize = lemmatizer.lemmatize(second_word_tokens_list[j])\n",
    "            if lemmatizer not in final_word_tokens_list:\n",
    "                if stem_lammatize != '':\n",
    "                    final_word_tokens_list.append(stem_lammatize)\n",
    "        for word_token in final_word_tokens_list:\n",
    "            tokens.append((word_token, i))\n",
    "    # Sort Tokens\n",
    "    tokens.sort(key=lambda a: a[0])\n",
    "\n",
    "\n",
    "preprocess_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2c4a7",
   "metadata": {},
   "source": [
    "1.3 Sort Tokens and Create Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32e0dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_index():\n",
    "    # create postings list\n",
    "    last_token = None\n",
    "    last_doc_id = None\n",
    "    for i in range(len(tokens)):\n",
    "        positional_postings = {}\n",
    "        positions = []\n",
    "        token_i, doc_id_i = tokens[i]\n",
    "        if token_i != last_token:\n",
    "            positional_postings[doc_id_i] = (positions, 0, None)\n",
    "            positional_index[token_i] = (1, positional_postings, None)\n",
    "        else:\n",
    "            if doc_id_i != last_doc_id:\n",
    "                positional_postings = positional_index[token_i][1]\n",
    "                positional_postings[doc_id_i] = (positions, 0)\n",
    "                positional_index[token_i] = (positional_index[token_i][0] + 1, positional_postings)\n",
    "        last_token = token_i\n",
    "        last_doc_id = doc_id_i\n",
    "    # create positional index\n",
    "    for i in range(len(content_dataset)):\n",
    "        word_tokens_list = hazm.word_tokenize(content_dataset[i])\n",
    "        for position, word_token in enumerate(word_tokens_list):\n",
    "            stem_lammatize = lemmatizer.lemmatize(word_token)\n",
    "            if stem_lammatize in positional_index.keys():\n",
    "                positional_postings = positional_index[stem_lammatize][1]\n",
    "                if i in positional_postings.keys():\n",
    "                    positions = positional_postings[i][0]\n",
    "                    positions.append(position)\n",
    "                    positional_postings[i] = (positions, positional_postings[i][1] + 1)\n",
    "                    positional_index[stem_lammatize] = (positional_index[stem_lammatize][0], positional_postings)\n",
    "\n",
    "\n",
    "create_positional_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88208dc3",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5516181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequencies in all documents:301\n",
      "docId=1 positions=[7]\n",
      "docId=3 positions=[215]\n",
      "docId=16 positions=[189]\n",
      "docId=52 positions=[224]\n",
      "docId=62 positions=[437]\n",
      "docId=65 positions=[184]\n",
      "docId=77 positions=[51]\n",
      "docId=116 positions=[357]\n",
      "docId=128 positions=[260]\n",
      "docId=140 positions=[654, 1351]\n",
      "docId=232 positions=[126]\n",
      "docId=274 positions=[139]\n",
      "docId=277 positions=[537, 621]\n",
      "docId=278 positions=[205]\n",
      "docId=291 positions=[84]\n",
      "docId=315 positions=[6]\n",
      "docId=350 positions=[65, 70]\n",
      "docId=376 positions=[376]\n",
      "docId=378 positions=[6, 196, 396]\n",
      "docId=435 positions=[624]\n",
      "docId=449 positions=[589, 656]\n"
     ]
    }
   ],
   "source": [
    "term = 'سجاد'\n",
    "print(\"Number of frequencies in all documents:\" + str(positional_index[term][0]))\n",
    "for index, (docID, positional_postings) in enumerate(positional_index[term][1].items()):\n",
    "    if index > 20:\n",
    "        break\n",
    "    positions = positional_postings[0]\n",
    "    print(\"docId=\" + str(docID) + \" positions=\" + str(positions))\n",
    "\n",
    "ranked_result_docs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a994312",
   "metadata": {},
   "source": [
    "1.4 Preprocess Query and Retrieving Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29f769ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query):\n",
    "    doc_id_with_last_phrase_position = {}\n",
    "\n",
    "    # Preprocess Query\n",
    "    split_query = query.split(\" \")\n",
    "    see_str = False\n",
    "    phrase_str = ''\n",
    "    phrase_list = []\n",
    "    not_list = []\n",
    "    and_list = []\n",
    "    for split_word in split_query:\n",
    "        if '\"' in split_word:\n",
    "            if not see_str:\n",
    "                phrase_str += split_word.split('\"')[1]\n",
    "                see_str = True\n",
    "            else:\n",
    "                phrase_str += \" \" + split_word.split('\"')[0]\n",
    "\n",
    "                see_str = False\n",
    "                phrase_list.append(phrase_str)\n",
    "                phrase_str = ''\n",
    "        elif see_str:\n",
    "            phrase_str += \" \" + split_word\n",
    "\n",
    "        elif '!' in split_word:\n",
    "            not_list.append(split_word.split('!')[1])\n",
    "\n",
    "        else:\n",
    "            and_list.append(split_word)\n",
    "    and_list_tokens = []\n",
    "    for i in range(len(and_list)):\n",
    "        word = normalizer.normalize(and_list[i])\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        and_list_tokens.append(word)\n",
    "    not_list_tokens = []\n",
    "    for i in range(len(not_list)):\n",
    "        word = normalizer.normalize(not_list[i])\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        not_list_tokens.append(word)\n",
    "    phrase_list_tokens = []\n",
    "    for i in range(len(phrase_list)):\n",
    "        phrase = normalizer.normalize(phrase_list[i])\n",
    "        phrase_list_tokens.append(phrase)\n",
    "\n",
    "    # intersect\n",
    "    for i in range(len(and_list_tokens)):\n",
    "        if and_list_tokens[i] in positional_index.keys():\n",
    "            result_docs = positional_index[and_list_tokens[i]][1].keys()\n",
    "            for doc in result_docs:\n",
    "                if doc not in ranked_result_docs.keys():\n",
    "                    ranked_result_docs[doc] = 1\n",
    "                else:\n",
    "                    ranked_result_docs[doc] = ranked_result_docs[doc] + 1\n",
    "    # not\n",
    "    for i in range(len(not_list_tokens)):\n",
    "        if not_list_tokens[i] in positional_index.keys():\n",
    "            result_docs = positional_index[not_list_tokens[i]][1].keys()\n",
    "            for doc in result_docs:\n",
    "                if doc in ranked_result_docs.keys():\n",
    "                    ranked_result_docs.pop(doc)\n",
    "\n",
    "    # phrase\n",
    "    for i in range(len(phrase_list_tokens)):\n",
    "        result_docs = []\n",
    "        phrase_words = phrase_list_tokens[i].split(\" \")\n",
    "        for word in phrase_words:\n",
    "            word_lemmatize = lemmatizer.lemmatize(word)\n",
    "            if word_lemmatize in positional_index.keys():\n",
    "                result_docs.append(positional_index[word_lemmatize][1])\n",
    "\n",
    "        for first_doc_id in result_docs[0].keys():\n",
    "            for second_doc_id in result_docs[1].keys():\n",
    "                if first_doc_id == second_doc_id:\n",
    "                    positions_list = []\n",
    "                    first_positions_list = result_docs[0][first_doc_id][0]\n",
    "                    second_positions_list = result_docs[1][second_doc_id][0]\n",
    "                    for position1 in first_positions_list:\n",
    "                        for position2 in second_positions_list:\n",
    "                            if position1 + 1 == position2:\n",
    "                                positions_list.append(position2)\n",
    "                    if len(positions_list) != 0:\n",
    "                        doc_id_with_last_phrase_position[first_doc_id] = positions_list\n",
    "        if len(doc_id_with_last_phrase_position) != 0:\n",
    "            for j in range(2, len(result_docs)):\n",
    "                new_doc_id_with_last_position = {}\n",
    "                for doc_id in result_docs[j].keys():\n",
    "                    new_positions_list = []\n",
    "                    if doc_id in doc_id_with_last_phrase_position.keys():\n",
    "                        positions_list = result_docs[j][doc_id][0]\n",
    "                        for position1 in doc_id_with_last_phrase_position[doc_id]:\n",
    "                            for position2 in positions_list:\n",
    "\n",
    "                                if position1 + 1 == position2:\n",
    "                                    new_positions_list.append(position2)\n",
    "                    if len(new_positions_list) != 0:\n",
    "                        new_doc_id_with_last_position[doc_id] = new_positions_list\n",
    "                doc_id_with_last_phrase_position = new_doc_id_with_last_position\n",
    "\n",
    "        for doc_id in doc_id_with_last_phrase_position.keys():\n",
    "            if doc_id not in ranked_result_docs.keys():\n",
    "                ranked_result_docs[doc_id] = 1\n",
    "            else:\n",
    "                ranked_result_docs[doc_id] = ranked_result_docs[doc_id] + 1\n",
    "    return doc_id_with_last_phrase_position, and_list_tokens\n",
    "\n",
    "\n",
    "ranked_result_docs = dict(sorted(ranked_result_docs.items(), key=lambda x: x[1], reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7f465",
   "metadata": {},
   "source": [
    "1.5 Show Result with Title and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4675844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(doc_id_with_last_phrase_position, and_list_tokens):\n",
    "    for i, (doc_id, number) in enumerate(ranked_result_docs.items()):\n",
    "        positions = []\n",
    "        sentences = []\n",
    "        for word in and_list_tokens:\n",
    "            if word in positional_index.keys():\n",
    "                if doc_id in positional_index[word][1].keys():\n",
    "                    for position in positional_index[word][1][doc_id][0]:\n",
    "                        positions.append(position)\n",
    "        if doc_id in doc_id_with_last_phrase_position.keys():\n",
    "            for position in doc_id_with_last_phrase_position[doc_id]:\n",
    "                positions.append(position)\n",
    "\n",
    "        if i < 5:\n",
    "            print(\"Document \" + str(doc_id) + \":\")\n",
    "\n",
    "            print(title_dataset[doc_id])\n",
    "\n",
    "            doc_words = hazm.word_tokenize(content_dataset[doc_id])\n",
    "            for count, word in enumerate(doc_words):\n",
    "                if count in positions:\n",
    "                    sentence = ''\n",
    "                    for k in range(count, -1, -1):\n",
    "                        if doc_words[k] != '.':\n",
    "                            if k in positions:\n",
    "                                positions.remove(k)\n",
    "                            sentence = doc_words[k] + ' ' + sentence\n",
    "                        else:\n",
    "                            break\n",
    "                    for j in range(count + 1, len(doc_words)):\n",
    "                        if doc_words[j] != '.':\n",
    "                            if j in positions:\n",
    "                                positions.remove(j)\n",
    "                            sentence = sentence + ' ' + doc_words[j]\n",
    "                        else:\n",
    "                            sentence += '.'\n",
    "                            break\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "            for num, sentence in enumerate(sentences):\n",
    "                print(\"\\n        Sentence \" + str(num + 1) + \":\")\n",
    "                print(sentence + '\\n')\n",
    "\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef62a5",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b984cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 86:\n",
      "نشست خبری جشنواره یکصد برای انتخاب برترین‌های قرن ورزش ایران برگزار شد\n",
      "\n",
      "        Sentence 1:\n",
      "رشته‌هایی که در آسیا مدال گرفتند یا سهمیه المپیک  گرفتند سه نامزد معرفی می‌کنند.\n",
      "\n",
      "Document 164:\n",
      "رستمیان: رنکینگ ما در کسب سهمیه المپیک مهم است/حضور در جام جهانی قطعی نیست\n",
      "\n",
      "        Sentence 1:\n",
      "وی گفت : امتیاز این مسابقات خیلی مهم است چون از روی رنکینگ سهمیه المپیک  توزیع می‌شود ، باید بتوانیم جایگاه خود را در رنکینگ حفظ کنیم.\n",
      "\n",
      "\n",
      "        Sentence 2:\n",
      "از همین حالا امتیاز مسابقات و رنکینگ ما در توزیع سهمیه المپیک  پاریس تاثیرگذار است.\n",
      "\n",
      "Document 228:\n",
      "گلخندان: مدال فروغی در المپیک مشوق خوبی برای همه بود/ نمی‌توان برای بازی‌های آسیایی قول داد\n",
      "\n",
      "        Sentence 1:\n",
      "وی در خصوص کسب سهمیه المپیک  از طریق رنکینگ جهانی ، گفت : از سال گذشته فدراسیون جهانی برای سهمیه المپیک رنکینگ را مدنظر قرار داد.\n",
      "\n",
      "\n",
      "        Sentence 2:\n",
      "کسانی که رنکینگ جهانی داشته_باشند با قوانین خاصی می‌توانند سهمیه المپیک  بگیرند اما همیشه شرکت در هر مسابقه جهانی در رنکینگ تاثیر داشته_است.\n",
      "\n",
      "Document 458:\n",
      "میراسماعیلی: دلسوزی دبیر نفع شخصی نیست و همه عدالت می خواهیم/دروغ می گویند ورزش، سیاسی نیست\n",
      "\n",
      "        Sentence 1:\n",
      "باید برنامه ریزی دقیق‌تر و تلاش بیشتری کنیم زیرا بازی‌های آسیایی و کسب سهمیه المپیک  را پیش رو داریم.\n",
      "\n",
      "Document 562:\n",
      "داورزنی: از وزارت ورزش پول نمی‌خواهیم/ حضور در جمع برترین‌ها زمان‌بر است\n",
      "\n",
      "        Sentence 1:\n",
      "نتوانستیم در المپیک ۲۰۰۸ و ۲۰۱۲ حضور داشته_باشیم ولی در گام سوم توانستم بعد از ۵۳ سال سهمیه المپیک  ۲۰۱۶ کسب کنیم.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query = 'باشگاه‌های فوتسال آسیا'\n",
    "# query = 'باشگاه‌های فوتسال !آسیا'\n",
    "query = '\"سهمیه المپیک\"'\n",
    "# query = '\"لیگ برتر\" !والیبال'\n",
    "# query = 'مایکل !جردن'\n",
    "documents_with_last_phrase_position, tokens_list = retrieve_documents(query)\n",
    "show_result(documents_with_last_phrase_position, tokens_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
