{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7fef21",
   "metadata": {},
   "source": [
    "# Information Retrival Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5f0c2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1- Positional Index Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36da158",
   "metadata": {},
   "source": [
    "1.1 Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b9af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hazm\n",
    "import math\n",
    "with open('IR_data_news_12k.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "content_dataset, url_dataset, title_dataset = [], [], []\n",
    "for _, data in json_data.items():\n",
    "    content_dataset.append(data[\"content\"])\n",
    "    url_dataset.append(data[\"url\"])\n",
    "    title_dataset.append(data[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c4805",
   "metadata": {},
   "source": [
    "1.2 Preprocess Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319be946",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = hazm.stopwords_list()\n",
    "punctuations = [')', '(', '>', '<', \"؛\", \"،\", '{', '}', \"؟\", ':', \"–\", '»', '\"', '«', '[', ']', '\"', '+', '=', '?', '/',\n",
    "                '//', '\\\\', '|', '!', '%', '&', '*', '$', '#', '؟', '*', '.', '_', '']\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "tokens = []\n",
    "for i in range(len(content_dataset)):\n",
    "    content_dataset[i] = normalizer.normalize(content_dataset[i])\n",
    "    first_word_tokens_list = hazm.word_tokenize(content_dataset[i])\n",
    "    second_word_tokens_list = []\n",
    "    final_word_tokens_list = []\n",
    "    for word_token in first_word_tokens_list:\n",
    "        if word_token not in stop_words and word_token not in punctuations:\n",
    "            second_word_tokens_list.append(word_token)\n",
    "    for j in range(len(second_word_tokens_list)):\n",
    "        stem_lammatize = lemmatizer.lemmatize(second_word_tokens_list[j])\n",
    "        if lemmatizer not in final_word_tokens_list:\n",
    "            if stem_lammatize != '':\n",
    "                final_word_tokens_list.append(stem_lammatize)\n",
    "    for word_token in final_word_tokens_list:\n",
    "        tokens.append((word_token, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcad6c1",
   "metadata": {},
   "source": [
    "1.3 Sort Tokens and Create Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548b5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort Tokens\n",
    "tokens.sort(key=lambda a: a[0])\n",
    "positional_index = {}\n",
    "\n",
    "# create postings list\n",
    "last_token = None\n",
    "last_doc_id = None\n",
    "for i in range(len(tokens)):\n",
    "    positional_postings = {}\n",
    "    positions = []\n",
    "    token_i, doc_id_i = tokens[i]\n",
    "    if token_i != last_token:\n",
    "        positional_postings[doc_id_i] = (positions, 0)\n",
    "        positional_index[token_i] = (1, positional_postings)\n",
    "    else:\n",
    "        if doc_id_i != last_doc_id:\n",
    "            positional_postings = positional_index[token_i][1]\n",
    "            positional_postings[doc_id_i] = (positions, 0)\n",
    "            positional_index[token_i] = (positional_index[token_i][0] + 1, positional_postings)\n",
    "    last_token = token_i\n",
    "    last_doc_id = doc_id_i\n",
    "# create positional index\n",
    "for i in range(len(content_dataset)):\n",
    "    word_tokens_list = hazm.word_tokenize(content_dataset[i])\n",
    "    positional_postings = {}\n",
    "    for position, word_token in enumerate(word_tokens_list):\n",
    "        stem_lammatize = lemmatizer.lemmatize(word_token)\n",
    "        token = None\n",
    "        if stem_lammatize in positional_index.keys():\n",
    "            positional_postings = positional_index[stem_lammatize][1]\n",
    "            if i in positional_postings.keys():\n",
    "                positions = positional_postings[i][0]\n",
    "                positions.append(position)\n",
    "                positional_postings[i] = (positions, positional_postings[i][1] + 1)\n",
    "                positional_index[stem_lammatize] = (positional_index[stem_lammatize][0], positional_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec6945",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b41d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequencies in all documents:301\n",
      "docId=1 positions=[7] ,docId=3 positions=[215] ,docId=16 positions=[189] ,docId=52 positions=[224] ,docId=62 positions=[437] ,docId=65 positions=[184] ,docId=77 positions=[51] ,docId=116 positions=[357] ,docId=128 positions=[260] ,docId=140 positions=[654, 1351] ,docId=232 positions=[126] ,docId=274 positions=[139] ,docId=277 positions=[537, 621] ,docId=278 positions=[205] ,docId=291 positions=[84] ,docId=315 positions=[6] ,docId=350 positions=[65, 70] ,docId=376 positions=[376] ,docId=378 positions=[6, 196, 396] ,docId=435 positions=[624] ,docId=449 positions=[589, 656] ,docId=468 positions=[28, 52] ,docId=472 positions=[140] ,docId=501 positions=[106, 288] ,docId=529 positions=[34] ,docId=531 positions=[30] ,docId=532 positions=[8] ,docId=535 positions=[9, 117, 120, 414, 773] ,docId=538 positions=[33] ,docId=544 positions=[600] ,docId=562 positions=[527] ,docId=569 positions=[8, 259] ,docId=576 positions=[16, 25] ,docId=577 positions=[8, 240, 477] ,docId=707 positions=[20, 27, 223, 448] ,docId=739 positions=[450] ,docId=756 positions=[17] ,docId=790 positions=[80] ,docId=872 positions=[132, 360, 607] ,docId=907 positions=[15] ,docId=935 positions=[131] ,docId=940 positions=[108, 112] ,docId=976 positions=[16, 36, 87, 180, 301, 348, 383] ,docId=1065 positions=[70, 73] ,docId=1146 positions=[9] ,docId=1170 positions=[69] ,docId=1207 positions=[160] ,docId=1237 positions=[157] ,docId=1252 positions=[117, 754] ,docId=1263 positions=[934] ,docId=1336 positions=[203, 330] ,docId=1375 positions=[100] ,docId=1388 positions=[65] ,docId=1393 positions=[314] ,docId=1394 positions=[64, 125] ,docId=1399 positions=[46] ,docId=1401 positions=[54, 141] ,docId=1415 positions=[144] ,docId=1444 positions=[24] ,docId=1487 positions=[42] ,docId=1505 positions=[38] ,docId=1515 positions=[32] ,docId=1527 positions=[159] ,docId=1578 positions=[36, 109, 291, 401] ,docId=1633 positions=[1576, 1612] ,docId=1646 positions=[27] ,docId=1647 positions=[196] ,docId=1672 positions=[30] ,docId=1719 positions=[15] ,docId=1801 positions=[551] ,docId=1816 positions=[762, 887, 966] ,docId=1819 positions=[329, 511, 518] ,docId=2076 positions=[878] ,docId=2079 positions=[50, 149, 437] ,docId=2110 positions=[479] ,docId=2112 positions=[278] ,docId=2121 positions=[35] ,docId=2127 positions=[8] ,docId=2129 positions=[9] ,docId=2133 positions=[9] ,docId=2189 positions=[286, 304, 429] ,docId=2200 positions=[153, 271] ,docId=2216 positions=[43] ,docId=2313 positions=[7, 172] ,docId=2327 positions=[221] ,docId=2341 positions=[142] ,docId=2355 positions=[9, 199, 415] ,docId=2358 positions=[219] ,docId=2408 positions=[555] ,docId=2485 positions=[1428] ,docId=2540 positions=[142] ,docId=2553 positions=[209] ,docId=2698 positions=[6, 248, 489] ,docId=2727 positions=[163] ,docId=2758 positions=[1416] ,docId=2807 positions=[77] ,docId=2829 positions=[13, 77] ,docId=2857 positions=[119] ,docId=2883 positions=[7] ,docId=2892 positions=[242] ,docId=2895 positions=[5] ,docId=2912 positions=[30, 235] ,docId=2939 positions=[65] ,docId=2998 positions=[7, 110] ,docId=3012 positions=[232, 663] ,docId=3021 positions=[9] ,docId=3037 positions=[353] ,docId=3040 positions=[14, 65] ,docId=3106 positions=[8, 407, 692, 978, 1255] ,docId=3110 positions=[46] ,docId=3172 positions=[31] ,docId=3195 positions=[329] ,docId=3208 positions=[93] ,docId=3218 positions=[41, 51, 118, 173, 206] ,docId=3221 positions=[8, 178, 344, 571] ,docId=3222 positions=[201, 303] ,docId=3228 positions=[32] ,docId=3247 positions=[68, 71] ,docId=3277 positions=[6] ,docId=3305 positions=[375] ,docId=3306 positions=[34] ,docId=3312 positions=[7] ,docId=3347 positions=[174] ,docId=3405 positions=[44] ,docId=3416 positions=[84, 107] ,docId=3457 positions=[1207] ,docId=3467 positions=[6, 39, 131, 212] ,docId=3469 positions=[166] ,docId=3473 positions=[6, 183] ,docId=3482 positions=[43] ,docId=3509 positions=[39] ,docId=3516 positions=[119] ,docId=3526 positions=[149, 197] ,docId=3528 positions=[6, 97] ,docId=3529 positions=[33] ,docId=3556 positions=[390, 476] ,docId=3594 positions=[59] ,docId=3603 positions=[25, 38] ,docId=3620 positions=[720, 739] ,docId=3630 positions=[88] ,docId=3646 positions=[52] ,docId=3672 positions=[173] ,docId=3691 positions=[81] ,docId=3694 positions=[560] ,docId=3732 positions=[227, 262] ,docId=3748 positions=[9] ,docId=3762 positions=[8, 126, 349, 518, 619, 816] ,docId=3767 positions=[44, 66, 268] ,docId=3776 positions=[72] ,docId=3782 positions=[148] ,docId=3875 positions=[553] ,docId=3882 positions=[60] ,docId=3887 positions=[159] ,docId=3916 positions=[107] ,docId=4002 positions=[321] ,docId=4042 positions=[5] ,docId=4071 positions=[830, 1018] ,docId=4094 positions=[489] ,docId=4104 positions=[8] ,docId=4146 positions=[352, 399] ,docId=4167 positions=[8, 133, 189, 315] ,docId=4173 positions=[70] ,docId=4176 positions=[205] ,docId=4177 positions=[9] ,docId=4186 positions=[9] ,docId=4187 positions=[9] ,docId=4214 positions=[56] ,docId=4236 positions=[172] ,docId=4252 positions=[578] ,docId=4311 positions=[115] ,docId=4317 positions=[30, 68] ,docId=4322 positions=[88] ,docId=4324 positions=[254, 305, 358] ,docId=4345 positions=[72, 322, 482, 679] ,docId=4376 positions=[155, 392] ,docId=4381 positions=[89] ,docId=4385 positions=[114, 323] ,docId=4387 positions=[10, 204, 432, 763] ,docId=4406 positions=[63] ,docId=4451 positions=[821] ,docId=4469 positions=[115, 230, 322] ,docId=4509 positions=[102] ,docId=4576 positions=[43] ,docId=4605 positions=[159] ,docId=4607 positions=[81] ,docId=4625 positions=[0] ,docId=4739 positions=[170] ,docId=4756 positions=[6, 117] ,docId=4817 positions=[29, 68] ,docId=4828 positions=[219, 266] ,docId=4870 positions=[29] ,docId=4873 positions=[553] ,docId=4906 positions=[6, 305] ,docId=4976 positions=[15, 95] ,docId=4992 positions=[6] ,docId=5009 positions=[125] ,docId=5013 positions=[249] ,docId=5022 positions=[161] ,docId=5029 positions=[221] ,docId=5054 positions=[508] ,docId=5097 positions=[8, 37, 277, 287] ,docId=5108 positions=[287, 345] ,docId=5142 positions=[6, 204] ,docId=5180 positions=[75] ,docId=5213 positions=[27] ,docId=5222 positions=[146, 200, 495] ,docId=5233 positions=[331] ,docId=5240 positions=[6, 38] ,docId=5254 positions=[426] ,docId=5262 positions=[404, 409] ,docId=5268 positions=[1969] ,docId=5338 positions=[536] ,docId=5361 positions=[249] ,docId=5421 positions=[8, 155] ,docId=5445 positions=[8, 271, 805, 1165, 1561, 1688, 2123, 2380] ,docId=5587 positions=[68] ,docId=5594 positions=[61] ,docId=5621 positions=[264] ,docId=5632 positions=[254] ,docId=5661 positions=[37, 52] ,docId=5690 positions=[6] ,docId=5718 positions=[20, 59] ,docId=5726 positions=[25, 58] ,docId=5729 positions=[8, 68, 333, 923] ,docId=5734 positions=[19, 59, 77, 174, 279] ,docId=5746 positions=[16] ,docId=5767 positions=[265] ,docId=5771 positions=[671] ,docId=5833 positions=[240, 613] ,docId=5857 positions=[71] ,docId=5864 positions=[139] ,docId=5885 positions=[187] ,docId=5896 positions=[1102, 1166] ,docId=5935 positions=[318] ,docId=5972 positions=[168] ,docId=5989 positions=[195, 247] ,docId=6037 positions=[6, 99] ,docId=6076 positions=[77] ,docId=6078 positions=[524] ,docId=6083 positions=[118] ,docId=6104 positions=[29] ,docId=6128 positions=[353] ,docId=6167 positions=[8, 108] ,docId=6170 positions=[207] ,docId=6178 positions=[217, 1188] ,docId=6217 positions=[341, 347] ,docId=6230 positions=[151] ,docId=6248 positions=[53] ,docId=6306 positions=[149] ,docId=6312 positions=[186] ,docId=6436 positions=[720, 784, 809, 826, 898, 931, 956, 975, 2727, 2772, 2794, 2819, 2984, 3409, 3517, 4551, 6524, 6551] ,docId=6508 positions=[170] ,docId=6551 positions=[210, 330] ,docId=6557 positions=[94, 207, 603] ,docId=6558 positions=[53, 68] ,docId=6569 positions=[134] ,docId=6570 positions=[52] ,docId=6587 positions=[38] ,docId=6701 positions=[262, 269] ,docId=6759 positions=[288, 300, 658, 667, 772, 956] ,docId=6817 positions=[304] ,docId=6843 positions=[10, 146, 281, 383] ,docId=6982 positions=[138] ,docId=6999 positions=[114] ,docId=7032 positions=[67, 138, 466] ,docId=7059 positions=[114] ,docId=7092 positions=[36] ,docId=7217 positions=[424] ,docId=7258 positions=[1306] ,docId=7480 positions=[1, 213, 363, 518, 703, 936] ,docId=7579 positions=[1379] ,docId=7689 positions=[238] ,docId=7741 positions=[143] ,docId=7765 positions=[526] ,docId=7796 positions=[310] ,docId=7819 positions=[308] ,docId=7830 positions=[285] ,docId=8265 positions=[615] ,docId=8539 positions=[1, 265, 403, 576] ,docId=8780 positions=[1147] ,docId=8828 positions=[55] ,docId=8832 positions=[9, 239, 382] ,docId=8856 positions=[92] ,docId=9239 positions=[595] ,docId=9845 positions=[1467] ,docId=9916 positions=[1443] ,docId=9925 positions=[11] ,docId=9933 positions=[285] ,docId=10082 positions=[32] ,docId=10139 positions=[2977] ,docId=10153 positions=[9, 137, 575, 932] ,docId=10279 positions=[131, 429] ,docId=10649 positions=[51, 577] ,docId=10711 positions=[839] ,docId=10825 positions=[39] ,docId=10905 positions=[282] ,docId=10924 positions=[32, 94] ,docId=11053 positions=[74] ,docId=11434 positions=[223] ,docId=11848 positions=[142] ,docId=12182 positions=[12] ,"
     ]
    }
   ],
   "source": [
    "term = 'سجاد'\n",
    "print(\"Number of frequencies in all documents:\" + str(positional_index[term][0]))\n",
    "# print(\"List of docId:\" + str(positional_index[term][1].keys()))\n",
    "for docID, positional_postings in positional_index[term][1].items():\n",
    "    positions = positional_postings[0]\n",
    "    print(\"docId=\" + str(docID) + \" positions=\" + str(positions) + \" ,\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9ff2b",
   "metadata": {},
   "source": [
    "1.4 Answering Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f769ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marhale 3\n",
    "# query = 'باشگاه‌های فوتسال آسیا'\n",
    "# query = 'باشگاه‌های فوتسال !آسیا'\n",
    "query = '\"سهمیه المپیک\"'\n",
    "# query = '\"لیگ برتر\" !والیبال'\n",
    "# query = 'مایکل !جردن'\n",
    "\n",
    "# split query\n",
    "split_query = query.split(\" \")\n",
    "see_str = False\n",
    "phrase_str = ''\n",
    "phrase_list = []\n",
    "not_list = []\n",
    "and_list = []\n",
    "for split_word in split_query:\n",
    "    if '\"' in split_word:\n",
    "        if not see_str:\n",
    "            phrase_str += split_word.split('\"')[1]\n",
    "            see_str = True\n",
    "        else:\n",
    "            phrase_str += \" \" + split_word.split('\"')[0]\n",
    "\n",
    "            see_str = False\n",
    "            phrase_list.append(phrase_str)\n",
    "            phrase_str = ''\n",
    "    elif see_str:\n",
    "        phrase_str += \" \" + split_word\n",
    "\n",
    "    elif '!' in split_word:\n",
    "        not_list.append(split_word.split('!')[1])\n",
    "\n",
    "    else:\n",
    "        and_list.append(split_word)\n",
    "and_list_tokens = []\n",
    "for i in range(len(and_list)):\n",
    "    word = normalizer.normalize(and_list[i])\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    and_list_tokens.append(word)\n",
    "not_list_tokens = []\n",
    "for i in range(len(not_list)):\n",
    "    word = normalizer.normalize(not_list[i])\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    not_list_tokens.append(word)\n",
    "phrase_list_tokens = []\n",
    "for i in range(len(phrase_list)):\n",
    "    phrase = normalizer.normalize(phrase_list[i])\n",
    "    phrase_list_tokens.append(phrase)\n",
    "\n",
    "ranked_result_docs = {}\n",
    "\n",
    "# intersect\n",
    "for i in range(len(and_list_tokens)):\n",
    "    if and_list_tokens[i] in positional_index.keys():\n",
    "        result_docs = positional_index[and_list_tokens[i]][1].keys()\n",
    "        for doc in result_docs:\n",
    "            if doc not in ranked_result_docs.keys():\n",
    "                ranked_result_docs[doc] = 1\n",
    "            else:\n",
    "                ranked_result_docs[doc] = ranked_result_docs[doc] + 1\n",
    "# not\n",
    "for i in range(len(not_list_tokens)):\n",
    "    if not_list_tokens[i] in positional_index.keys():\n",
    "        result_docs = positional_index[not_list_tokens[i]][1].keys()\n",
    "        for doc in result_docs:\n",
    "            if doc in ranked_result_docs.keys():\n",
    "                ranked_result_docs.pop(doc)\n",
    "\n",
    "# phrase\n",
    "doc_id_with_last_phrase_position = {}\n",
    "for i in range(len(phrase_list_tokens)):\n",
    "    result_docs = []\n",
    "    phrase_words = phrase_list_tokens[i].split(\" \")\n",
    "    for word in phrase_words:\n",
    "        word_lemmatize = lemmatizer.lemmatize(word)\n",
    "        if word_lemmatize in positional_index.keys():\n",
    "            result_docs.append(positional_index[word_lemmatize][1])\n",
    "\n",
    "    for first_doc_id in result_docs[0].keys():\n",
    "        for second_doc_id in result_docs[1].keys():\n",
    "            if first_doc_id == second_doc_id:\n",
    "                positions_list = []\n",
    "                first_positions_list = result_docs[0][first_doc_id][0]\n",
    "                second_positions_list = result_docs[1][second_doc_id][0]\n",
    "                for position1 in first_positions_list:\n",
    "                    for position2 in second_positions_list:\n",
    "                        if position1 + 1 == position2:\n",
    "                            positions_list.append(position2)\n",
    "                if len(positions_list) != 0:\n",
    "                    doc_id_with_last_phrase_position[first_doc_id] = positions_list\n",
    "    if len(doc_id_with_last_phrase_position) != 0:\n",
    "        for j in range(2, len(result_docs)):\n",
    "            new_doc_id_with_last_position = {}\n",
    "            for doc_id in result_docs[j].keys():\n",
    "                new_positions_list = []\n",
    "                if doc_id in doc_id_with_last_phrase_position.keys():\n",
    "                    positions_list = result_docs[j][doc_id][0]\n",
    "                    for position1 in doc_id_with_last_phrase_position[doc_id]:\n",
    "                        for position2 in positions_list:\n",
    "\n",
    "                            if position1 + 1 == position2:\n",
    "                                new_positions_list.append(position2)\n",
    "                if len(new_positions_list) != 0:\n",
    "                    new_doc_id_with_last_position[doc_id] = new_positions_list\n",
    "            doc_id_with_last_phrase_position = new_doc_id_with_last_position\n",
    "\n",
    "    for doc_id in doc_id_with_last_phrase_position.keys():\n",
    "        if doc_id not in ranked_result_docs.keys():\n",
    "            ranked_result_docs[doc_id] = 1\n",
    "        else:\n",
    "            ranked_result_docs[doc_id] = ranked_result_docs[doc_id] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b6bc0",
   "metadata": {},
   "source": [
    "1.5 Show Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104a7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 86:\n",
      "نشست خبری جشنواره یکصد برای انتخاب برترین‌های قرن ورزش ایران برگزار شد\n",
      "\n",
      "        Sentence 1:\n",
      "رشته‌هایی که در آسیا مدال گرفتند یا سهمیه المپیک  گرفتند سه نامزد معرفی می‌کنند.\n",
      "\n",
      "Document 164:\n",
      "رستمیان: رنکینگ ما در کسب سهمیه المپیک مهم است/حضور در جام جهانی قطعی نیست\n",
      "\n",
      "        Sentence 1:\n",
      "وی گفت : امتیاز این مسابقات خیلی مهم است چون از روی رنکینگ سهمیه المپیک  توزیع می‌شود ، باید بتوانیم جایگاه خود را در رنکینگ حفظ کنیم.\n",
      "\n",
      "\n",
      "        Sentence 2:\n",
      "از همین حالا امتیاز مسابقات و رنکینگ ما در توزیع سهمیه المپیک  پاریس تاثیرگذار است.\n",
      "\n",
      "Document 228:\n",
      "گلخندان: مدال فروغی در المپیک مشوق خوبی برای همه بود/ نمی‌توان برای بازی‌های آسیایی قول داد\n",
      "\n",
      "        Sentence 1:\n",
      "وی در خصوص کسب سهمیه المپیک  از طریق رنکینگ جهانی ، گفت : از سال گذشته فدراسیون جهانی برای سهمیه المپیک رنکینگ را مدنظر قرار داد.\n",
      "\n",
      "\n",
      "        Sentence 2:\n",
      "کسانی که رنکینگ جهانی داشته_باشند با قوانین خاصی می‌توانند سهمیه المپیک  بگیرند اما همیشه شرکت در هر مسابقه جهانی در رنکینگ تاثیر داشته_است.\n",
      "\n",
      "Document 458:\n",
      "میراسماعیلی: دلسوزی دبیر نفع شخصی نیست و همه عدالت می خواهیم/دروغ می گویند ورزش، سیاسی نیست\n",
      "\n",
      "        Sentence 1:\n",
      "باید برنامه ریزی دقیق‌تر و تلاش بیشتری کنیم زیرا بازی‌های آسیایی و کسب سهمیه المپیک  را پیش رو داریم.\n",
      "\n",
      "Document 562:\n",
      "داورزنی: از وزارت ورزش پول نمی‌خواهیم/ حضور در جمع برترین‌ها زمان‌بر است\n",
      "\n",
      "        Sentence 1:\n",
      "نتوانستیم در المپیک ۲۰۰۸ و ۲۰۱۲ حضور داشته_باشیم ولی در گام سوم توانستم بعد از ۵۳ سال سهمیه المپیک  ۲۰۱۶ کسب کنیم.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranked_result_docs = dict(sorted(ranked_result_docs.items(), key=lambda x: x[1], reverse=True))\n",
    "for i, (doc_id, number) in enumerate(ranked_result_docs.items()):\n",
    "    positions = []\n",
    "    sentences = []\n",
    "    for word in and_list_tokens:\n",
    "        if word in positional_index.keys():\n",
    "            if doc_id in positional_index[word][1].keys():\n",
    "                for position in positional_index[word][1][doc_id][0]:\n",
    "                    positions.append(position)\n",
    "    if doc_id in doc_id_with_last_phrase_position.keys():\n",
    "        for position in doc_id_with_last_phrase_position[doc_id]:\n",
    "            positions.append(position)\n",
    "\n",
    "    if i < 5:\n",
    "        print(\"Document \" + str(doc_id) + \":\")\n",
    "\n",
    "        print(title_dataset[doc_id])\n",
    "\n",
    "        doc_words = hazm.word_tokenize(content_dataset[doc_id])\n",
    "        for count, word in enumerate(doc_words):\n",
    "            if count in positions:\n",
    "                sentence = ''\n",
    "                for k in range(count, -1, -1):\n",
    "                    if doc_words[k] != '.':\n",
    "                        if k in positions:\n",
    "                            positions.remove(k)\n",
    "                        sentence = doc_words[k] + ' ' + sentence\n",
    "                    else:\n",
    "                        break\n",
    "                for j in range(count + 1, len(doc_words)):\n",
    "                    if doc_words[j] != '.':\n",
    "                        if j in positions:\n",
    "                            positions.remove(j)\n",
    "                        sentence = sentence + ' ' + doc_words[j]\n",
    "                    else:\n",
    "                        sentence += '.'\n",
    "                        break\n",
    "\n",
    "                sentences.append(sentence)\n",
    "        for num, sentence in enumerate(sentences):\n",
    "            print(\"\\n        Sentence \" + str(num + 1) + \":\")\n",
    "            print(sentence + '\\n')\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
